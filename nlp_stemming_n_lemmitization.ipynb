{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp stemming_n_lemmitization",
      "provenance": [],
      "authorship_tag": "ABX9TyMaPe3ehEvo1Ub3tBsE1V7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raviyadav44/Nlp_practice/blob/main/nlp_stemming_n_lemmitization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NlZgJ4FlFPB"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xY85ililHoN"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0xa6t4blVNb"
      },
      "source": [
        "paragraph='''Right now we’re in a time where a lot of\r\n",
        "creators think YouTube is a very\r\n",
        "sustainable platform and I want to be a\r\n",
        "youtuber so let me be rich and famous on\r\n",
        "YouTube when I started in 2010 that was\r\n",
        "not a thing you know I was the last\r\n",
        "person to catch on to YouTube all my\r\n",
        "friends are watching this thing and I\r\n",
        "was like what are you guys wasting your\r\n",
        "time with on this video platform thing\r\n",
        "it’s gonna disappear and I spontaneously\r\n",
        "one day posted a video not because I\r\n",
        "thought it could be my job not because I\r\n",
        "could thought I could make money from it\r\n",
        "but literally because I was sad so the\r\n",
        "story is that I was getting my\r\n",
        "psychology degree and I wasn’t\r\n",
        "passionate about it and I was doing\r\n",
        "everything my sister had done before me\r\n",
        "you know I was following her footsteps\r\n",
        "blindly and after I graduated my dad was\r\n",
        "like great now get your master’s degree\r\n",
        "and I was like okay this is I don’t\r\n",
        "think I could do this anymore I don’t\r\n",
        "I’m not passion about any of these\r\n",
        "things and so in that little funk I\r\n",
        "discovered YouTube\r\n",
        "it’s kind of like the stars aligning\r\n",
        "perfectly and I posted a first video\r\n",
        "really spontaneously it was a spoken\r\n",
        "word piece about religion it was very\r\n",
        "very very bad and it got 70 views and I\r\n",
        "think I just fell in love with having a\r\n",
        "goal during that downtime and learning\r\n",
        "something new so I think I wasn’t\r\n",
        "trained at I reflect back in my life and\r\n",
        "I think about all the times I really\r\n",
        "became a better person or a smarter\r\n",
        "person it’s because of something that\r\n",
        "was very hurtful or painful or an\r\n",
        "unpleasant experience and well we’re all\r\n",
        "gonna get heartbroken over and over\r\n",
        "again we’re all gonna deal with failures\r\n",
        "all over again but it’s how we think\r\n",
        "about those moments in that moment allow\r\n",
        "myself to be upset when I’m heartbroken\r\n",
        "and allow myself to feel failure but not\r\n",
        "be down in the dumps about it for too\r\n",
        "long not be like oh well I’m worthless\r\n",
        "or oh well this is just how it’s meant\r\n",
        "to be in the world’s against me getting\r\n",
        "hurt efficiently means were hurt how can\r\n",
        "we efficiently learn a lesson in this\r\n",
        "moment what could I have done\r\n",
        "differently what could I expect it\r\n",
        "differently how could I have reacted\r\n",
        "differently so that the next time you\r\n",
        "will be heartbroken what you will be\r\n",
        "it’s inevitable you\r\n",
        "to deal with it think I learned how to\r\n",
        "be happy because I know what unhappiness\r\n",
        "felt like and I was able to dissect all\r\n",
        "the reasons I was unhappy you know\r\n",
        "paying attention to all the signs my\r\n",
        "mind and body were giving me say we’re\r\n",
        "not doing the right thing and I think\r\n",
        "having so much experience in that dark\r\n",
        "phase really helps you become better'''"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AEn9-psqzbv"
      },
      "source": [
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Bjy3virHX5"
      },
      "source": [
        "#words=nltk.word_tokenize(paragraph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYPbOs6wrLCm"
      },
      "source": [
        "import re\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9kInQMArmK4"
      },
      "source": [
        "#stemmer=PorterStemmer()\r\n",
        "# Stemming\r\n",
        "lemmitizar=WordNetLemmatizer()\r\n",
        "corpus=[]\r\n",
        "for i in range(len(sentences)):\r\n",
        "   review = re.sub('[^a-zA-Z]', ' ', sentences[i])\r\n",
        "   review = review.lower()\r\n",
        "   review = review.split()\r\n",
        "   review = [lemmitizar.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\r\n",
        "   review = ' '.join(review)\r\n",
        "   corpus.append(review)\r\n",
        "   "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzb_OjaHvMiW"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "cv=CountVectorizer(max_features=2500)\r\n",
        "x=cv.fit_transform(corpus).toarray()\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XfQN5CqvymJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b677cb53-1b8f-4f2e-d705-aeea20a82378"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 7, 1, 1, 1, 1, 2, 2, 3, 1,\n",
              "        1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1,\n",
              "        1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1,\n",
              "        6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1,\n",
              "        1, 1, 1, 3, 1, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2,\n",
              "        1, 2, 1, 1, 1, 1, 5, 8, 2, 4, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1,\n",
              "        1, 1, 4, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp23GvgRczbA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}